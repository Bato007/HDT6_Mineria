{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "seed = random.seed(123)\n",
    "number_clusters = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis Exploratorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable classification\n",
    "train_data = pd.read_csv('./data/train.csv', encoding = \"ISO-8859-1\")\n",
    "test_data = pd.read_csv('./data/test.csv', encoding = \"ISO-8859-1\")\n",
    "variables = pd.read_csv('./data/variables.txt', encoding = \"ISO-8859-1\")\n",
    "quant_vars = list(variables.loc[(variables['Clasification'] == 'Cuantitativa')]['Variable'].values)\n",
    "quali_vars = list(variables.loc[(variables['Clasification'] == 'Cualitativa')]['Variable'].values)[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analizando las variables numericas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[quant_vars].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in quant_vars:\n",
    "    data = train_data[var].dropna(how='all', axis=0)\n",
    "    \n",
    "    # Gráfico\n",
    "    sns.displot(data, kde=True)\n",
    "\n",
    "    # Mostrando normalidad\n",
    "    print('\\033[1m' + var + '\\033[0m' + ': Kurtosis:', stats.kurtosis(data), 'Skewness:', stats.skew(data), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analizando las variables categoricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in quali_vars:\n",
    "  plt.figure(figsize=(20,5))\n",
    "  train_data[var].value_counts().plot(kind='bar')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analizando la variable de interes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skewness and kurtosis\n",
    "print('Skewness: %f' % train_data['SalePrice'].skew())\n",
    "print('Kurtosis: %f' % train_data['SalePrice'].kurt())\n",
    "print('\\n---Describe---')\n",
    "train_data['SalePrice'].describe([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.65, 0.7, 0.8, 0.9, 0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat,p = stats.shapiro(train_data[[\"SalePrice\"]].dropna())\n",
    "print('Kolmogorov-Smirnov:\\np=%f\\n'% p)\n",
    "ks_statistic, p_value = diag.lilliefors(train_data[[\"SalePrice\"]].dropna())\n",
    "print('Lilliefors:\\nks=%f\\np=%f'%(ks_statistic,p_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.displot(train_data['SalePrice'], kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10 #number of variables for heatmap\n",
    "corrmat = train_data.corr()\n",
    "cols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\n",
    "cm = np.corrcoef(train_data[cols].values.T)\n",
    "sns.set(font_scale=1.25)\n",
    "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obteniendo la relacion entre las variables mas significativas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(train_data[cols],hue=\"SalePrice\")\n",
    "plt.show()\n",
    "# quant_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analizando data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize(row, option): \n",
    "    if (option == 0):\n",
    "        if row['SalePrice'] > 0 and row['SalePrice'] <= 179280:\n",
    "            return 1\n",
    "        return 0\n",
    "    elif (option == 1):\n",
    "        if (row['SalePrice'] > 179280 and row['SalePrice'] < 326100):\n",
    "            return 1\n",
    "        return 0\n",
    "    elif (option == 2):\n",
    "        if (row['SalePrice'] >= 326100):\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "train_data['isMedium'] = train_data.apply(lambda row: categorize(row, 1), axis=1)\n",
    "train_data['isExpensive'] = train_data.apply(lambda row: categorize(row, 2), axis=1)\n",
    "\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isLow isMedium isExpensive\n",
    "result = 'isMedium'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copied_train_data = train_data.copy()\n",
    "copied_train_data = copied_train_data.fillna(0)\n",
    "\n",
    "\n",
    "y = copied_train_data.pop(result) #La variable respuesta\n",
    "X = copied_train_data[quant_vars] #El resto de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "70% de entrenamiento y 30% prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test,y_train, y_test = train_test_split(X, y,test_size=0.3,train_size=0.7, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creando el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logReg = LogisticRegression(solver='liblinear')\n",
    "logReg.fit(X_train,y_train)\n",
    "y_pred = logReg.predict(X_test)\n",
    "cm = confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resultados esperados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=accuracy_score(y_test,y_pred)\n",
    "precision =precision_score(y_test, y_pred,average='micro')\n",
    "recall =  recall_score(y_test, y_pred,average='micro')\n",
    "f1 = f1_score(y_test,y_pred,average='micro')\n",
    "print('Matriz de confusión para detectar virginica\\n',cm)\n",
    "print('Accuracy: ',accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on the train dataset\n",
    "cross_validate(logReg, X_train, y_train, return_train_score=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grafica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x=X_test[\"SalePrice\"], y=y_pred, data=train_data, logistic=True, ci=None)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c33184972b48748d94ddd34e441bc25e543b9b1491b698e37ff4356c126c0090"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
